<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on nishihi6</title>
    <link>https://nishihi6.github.io/blog/posts/</link>
    <description>Recent content in Posts on nishihi6</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <lastBuildDate>Sat, 14 Oct 2023 23:00:08 +0900</lastBuildDate><atom:link href="https://nishihi6.github.io/blog/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CG×ML #3 DRを可能にするMitsuba3の中身 [Devlog #013]</title>
      <link>https://nishihi6.github.io/blog/posts/devlog_cg-ml_03/</link>
      <pubDate>Sat, 14 Oct 2023 23:00:08 +0900</pubDate>
      
      <guid>https://nishihi6.github.io/blog/posts/devlog_cg-ml_03/</guid>
      <description>微分可能レンダリングを行うMitsuba3の仕組み Dr.Jit（ジャストインタイムコンパイラ） ここでは微分可能なレンダリングのためのジャストインタイムコンパイラについての論文1を理解する
Wenzel Jakob, Sébastien Speierer, Nicolas Roussel, and Delio Vicini. 2022. Dr.Jit: A Just-In-Time Compiler for Differentiable Rendering. In Transactions on Graphics (Proceedings of SIGGRAPH) 41(4).
Dr.Jitとは 物理ベース微分可能レンダリング（PBDR）のためのジャストインタイムコンパイラ
レンダリングアルゴリズムを実行するとグラフを生成
グラフとは 算術、ループ、レイトレーシングの操作、およびレンダリングアルゴリズムとシーンオブジェクト（形状、BSDF、テクスチャ、エミッターなど）間の情報を交換する多態的な呼び出し
Dr.Jitは提供されるシーンにグラフを特化させ、LLVMまたはOptiXを介してデータ並列カーネル（megakernel）にコンパイルする
通常のレンダリングのほか、微分シミュレーション（differential simulations）を動的にコンパイルする
大きな微分タスクを一連の漸近的なステップに分解するため、自動微分（automatic differentiation）の細かい実行をサポート
画像空間における摂動（perturbation）を計算してデバッグやvisualizationに利用するForwardモードと、たくさんの未知数を同次最適化するためのパラメータ空間の導関数を提供するReverseモードを実装している
パラメータ空間の例 壁紙のテクセル（テクスチャ空間の基本単位）
目的 Python等の高レベル言語で記述されたシミュレーションコードの実行を追跡し、どの部分のコードがどのように動作しているのか、どの部分が計算負荷が高いのかなどの情報をもとに、コードを効率的に最適化・変換し、高速なデータ並列カーネルを生成する
微分の過程をより効率的に、かつ細かい制御のもとで行うことをサポートし、微分可能なレンダリングアルゴリズムの開発を簡易化する（効率的な方法としてシミュレーションの微分を微分のシミュレーションに変える）
&#34;シミュレーションの微分を微分のシミュレーションに変える&#34;とは Derivative of a simulation（シミュレーションの微分）：
シミュレーションが入力に対してどのように変化するかを示す
例えば、物理ベースのレンダリングの場合、光の強度や物質の性質などの入力パラメータに微小な変化を加えたときの出力画像の変化量がこれに該当する
Simulation of the derivative（微分のシミュレーション）：
微分そのものを新しいシミュレーションとして扱う考え方
つまり、入力の変化に対する出力の変化を直接シミュレートすることで、微分の効果を視覚化や解析するためのツールとして利用する
この変換は重要。なぜなら、 多くのアルゴリズムや最適化手法は、微分の情報を使用して動作する
微分可能なレンダリングは、シーンのパラメータを最適化するための勾配情報を提供する能力が求められる
この勾配情報は、シーンの照明や材料の性質、カメラの位置などのパラメータを調整して、目的の画像や効果に近づけるために使用される
Dr.Jitは自動微分のプロセスを細かく制御してこの変換を支援する また、Dr.Jitはデータの依存関係をグローバルに追跡し、最終的に勾配に影響を与えない計算（冗長なシミュレーション）を削除することで効率を上げる
Introduction 物理ベースの微分可能レンダリング（PBDR） PBDRは、任意のシーンパラメータに基づいて光輸送シミュレーションを微分できる
勾配ベースの最適化と組み合わせ、多くの未知数を持つ非線形の問題を解決する
この技術は、画像の逆解析が必要な多くの分野での利用が期待される
PBDRの課題 実用的な実装が難しい</description>
    </item>
    
    <item>
      <title>量子情報工学メモ #1 量子力学 [Memo #003]</title>
      <link>https://nishihi6.github.io/blog/posts/memo_qm_01/</link>
      <pubDate>Thu, 12 Oct 2023 19:45:54 +0900</pubDate>
      
      <guid>https://nishihi6.github.io/blog/posts/memo_qm_01/</guid>
      <description>量子コンピューティングを学ぶ 我々が目指している未来を少しでも理解できるようになることが、今後の自らの興味やスキルを発展させていくことで重要と考えたため、量子情報について学びます（CV・CGとは現状関わりの薄い分野ですが…）
背景 2050年頃（自分の子供が社会で活躍し始める頃）には、従来の情報通信技術に量子インターネットや量子センサなどの量子技術が加わり、量子コンピュータを自由自在に組み合わせて使いこなす「量子前提社会」が到来すると考えられている1
情報化社会が到来し、インターネットやコンピュータが多くの社会課題を解決してきたが、これの延長線上にあるものとして、量子情報技術はより難しい課題を解決すると考えられる
量子前提社会で必要とされる誤り耐性量子コンピュータの大規模化の実装は、操作精度や実装精度などに何万倍もの向上が必要になるが、原理的に実現を妨げる障壁は無い
また、大規模化を達成した誤り耐性量子コンピュータの実現は、内閣府が2020年に設定したムーンショット計画でも目標とされている2
※誤り耐性型汎用量子コンピュータ 大規模な集積化を実現しつつ、様々な用途に応用する上で十分な精度を保証できる量子コンピュータ ※ムーンショット計画 従来技術の延長を担い、より大胆な発想に基づく挑戦的な研究開発の推進を目的とした計画（名前の由来はケネディ大統領のアポロ計画に関する演説で用いられた言葉からきている） 関連動画 量子力学（Quantum Mechanics） システムと実験 - Systems and Experiments Quantum Mechanics Is Different Spins and Qubits An Experiment Experiments Are Never Gentle Proposition 古典的な命題のテスト - Testing Classical Propositions 量子的な命題のテスト - Testing Quantum Proposition Mathematical Interlude: Complex Number Mathematical Interlude: Vector Spaces 公理 - Axioms 関数と列ベクトル - Functions and Column Vectors ブラとケット - Bras and Kets 内積 - Inner Products 正規直交基底 - Orthonormal Bases 量子状態 - Quantumn States 状態とベクトル - States and Vectors スピン状態の表現 - Representing Spin States Along the x Axis Along the y Axis パラメータのカウント - Counting Parameters スピン状態を列ベクトルとして表現 - Representing Spin States as Column Vectors これまでのまとめ - Putting It All Together 量子力学の原理 - Principles of Quantum Mechanics 数学上の補説：線形演算子 - Mathematical Interlude: Liner Operators 機会と行列 - Machines and Matrices 固有値と固有ベクトル - Eigenvalues and EigenVectors エルミート共役 - Hermitian Conjugation エルミート演算子 - Hermitian Operators エルミート演算子と正規直交基底 - Hermitian Operators and Orthonormal Bases グラム・シュミット法 - The Gram-Schmidt Procedure 原理 - The Principles スピン演算子 - An Example: Spin Operator スピン演算子の構成 - Constructing Spin Operators よくある誤解 - A Common Misconception 3-ベクトル演算子の再考 - 3-Vector Operators Revisited 計算を行って結果を得る - Reaping the Results スピン偏極の原理 - The Spin-Polarization Principle 時間と変化 - Time and Change 古典力学の考え方 - A Classical Reminder ユニタリー - Unitarity 時間発展演算子</description>
    </item>
    
    <item>
      <title>DirectX12の描画処理 #5 [Devlog #012]</title>
      <link>https://nishihi6.github.io/blog/posts/devlog_dx12_05/</link>
      <pubDate>Wed, 11 Oct 2023 23:55:00 +0900</pubDate>
      
      <guid>https://nishihi6.github.io/blog/posts/devlog_dx12_05/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CG×ML #2 勾配ベースの最適化（Mitsuba3） [Devlog #011]</title>
      <link>https://nishihi6.github.io/blog/posts/devlog_cg-ml_02/</link>
      <pubDate>Sat, 07 Oct 2023 15:36:49 +0900</pubDate>
      
      <guid>https://nishihi6.github.io/blog/posts/devlog_cg-ml_02/</guid>
      <description>勾配ベースの最適化（Mitsuba3） Mitsuba3の動作確認 Mitsuba3はPyPIからpip経由でインストールすることが推奨されている 公式ドキュメントはこちら
Mitsuba3の使い方（Python） インポート import mitsuba as mi バリアントの選択 バリアントについての詳細
mi.variants() [&#39;scalar_rgb&#39;, &#39;scalar_spectral&#39;, &#39;cuda_ad_rgb&#39;, &#39;llvm_ad_rgb&#39;] この4つ以外のバリアントを必要とする場合は、pipではなく自力でコンパイルする必要がある（ソースからのコンパイルに関するドキュメントはこちら）
mi.set_variant(&amp;#34;scalar_rgb&amp;#34;) 計算バックエンド（Computational backend）の指定 scalar：旧Mitsubaと同様にCPU上で浮動小数点演算を実行する（一度に個々のレイを処理するモードであるため、コンパイルエラーの修正やレンダラーのデバッグに適している）
cuda：Dr.JITが計算をCUDAカーネルに変換してGPUにオフロードする（GPUレイトレーシングにNVIDIAのOptiXライブラリを使用する）
llvm：Dr.JITが計算を並列CPUカーネルにコンパイルする（LLVMコンパイラフレームワークを使用する）（※ NVIDIA GPUを持っていない場合の代替手段）
自動微分（Automatic differentiation）の指定 _ad：自動微分を有効にする（cudaとllvmモードで指定可能）
主に微分可能レンダリングを行う際に利用するモードで、光の動きを含めた複雑な逆問題を解く
色の表現（Color representation）の指定 mono：単色ベースの色表現
rgb：RGBベースの色表現
spectral：可視域をカバーする完全なスペクトルカラー表現
偏光（Polarization）の指定 偏光の追跡が必要な場合に指定する追加オプション
偏光は逆問題を解決するための強力なツールである
精度（Precision）の指定 _double：通常の演算で設定している単精度（32bit）から倍精度（64bit）に設定を変更する（EmbreeとOptiXが倍精度をサポートしていないことを考慮した設定が必要な場合がある）
シーンのロード scene = mi.load_file(&amp;#34;../scenes/cbox.xml&amp;#34;) Mitsuba3ではシーンの記述にXMLベースの形式を利用する（シーンXMLファイルフォーマットに関するドキュメントはこちら）
シーンのレンダリング image = mi.render(scene, spp=256) render()関数を使用してシーンをレンダリング
render()は線形RGB色空間テンソル（NumPy配列に似たmi.TensorXf）を返す
（APIリファレンスはこちら ※サイトが重いので注意）
mi.Bitmap(image) レンダリング画像の表示
matplotlibを使っても画像を表示できる import matplotlib.pyplot as plt plt.axis(&amp;#34;off&amp;#34;) plt.imshow(image ** (1.0 / 2.2)); レンダリング画像の保存 mi.util.write_bitmap(&amp;#34;my_first_render.png&amp;#34;, image) mi.</description>
    </item>
    
    <item>
      <title>CG×ML #1 微分可能レンダリング [Devlog #010]</title>
      <link>https://nishihi6.github.io/blog/posts/devlog_cg-ml_01/</link>
      <pubDate>Wed, 04 Oct 2023 16:03:45 +0900</pubDate>
      
      <guid>https://nishihi6.github.io/blog/posts/devlog_cg-ml_01/</guid>
      <description>微分可能レンダリング（differential rendering） 微分可能レンダリングとは 微分可能レンダリングは、コンピュータグラフィックスと機械学習を組み合わせた技術であり、2D画像から3D空間のパラメータを推定することを目的としている
通常のレンダリングでは、3Dモデル、カメラ位置、ライティング条件などのパラメータを利用して2D画像を生成するが、微分可能レンダリングはこのレンダリングのプロセスを逆に行う
2D画像から3D情報への変換の精度を向上させるために、レンダリング画像と入力画像（教師データ）との差分を計算し、ネットワーク全体を学習する手法として使用される この場合、教師画像データはカメラパラメータ（撮影された位置と方向）がわかっているものとし、レンダリングの際にはそのパラメータと同じような仮想カメラを設定してレンダリング画像を生成することになる
特に3次元形状の教師データが手に入らない場合に効果的で、さまざまな物体カテゴリにも適応可能である
一方で、3Dデータ（3次元形状やポーズなど）を機械学習タスクの教師データとして用いる学習は&amp;quot;3D supervision&amp;quot;と呼ばれる こちらは3D教師データを大量に用意しなければならないという点であまり現実的な選択肢ではない
微分可能レンダラとは レンダリング画像における入力画像（教師データ）との差分を誤差逆伝播させてニューラルネットを学習させるには、レンダリング箇所が微分可能である必要がある
つまり、レンダリング画像と入力画像（教師データ）との差分からパラメータの更新ができる（ロスからBackwardできる）レンダラを微分可能レンダラという
勾配ベースの最適化アルゴリズム（SGD、Adam等）で3Dシーンを最適化できるレンダラ
最適化する3Dシーンの例 頂点位置 カメラポーズ（姿勢） テクスチャ ライト その他、レンダリングに関する概念ほぼ全て 参考：微分可能レンダラのつくりかた 参考：ニューラルネット3D表現に対する微分可能レンダラー 参考：微分可能レンダリング
3次元構造の表現方法（研究発展中） ボクセル（Voxel） 点群（Point） メッシュ（Mesh） +α ニューラルネットワーク表現（ニューラル場） +α 点群-&amp;gt;ガウス メッシュ（Mesh） メッシュ表現を用いる場合は、何らかの初期形状を仮定して、それを変形させることで3次元構造の再構成を行う手法が主流である
レンダリング方式（研究発展中） ラスタライズ レイトレーシング +α Radiance Field +α 微分可能ガウスラスタライズ ラスタライズは視界の範囲だけ、レイトレーシングは視界範囲外の考慮してレンダリングする
参考：レイトレーシングとラスタライズの違い - NVIDIA
ラスタライズのアプローチ メッシュの微分可能ラスタライズには2つのアプローチがある
レンダリング画像を変えずに擬似的な勾配を計算（逆伝搬を工夫） レンダリング画像をぼやかすことで最適化に有用な勾配を得る（順伝搬を工夫） レイトレーシングのアプローチ 物理ベース微分可能レンダリング
計算速度が遅く、深層学習（機械学習）と組み合わせるのは難しいのが現状
Neural Radiance Field（NeRF）のアプローチ レンダリング方程式（詳細）を逆問題として解くが、ピクセルの色の決定にボリュームレンダリングを用いる
NeRF（neural radiance field）のボリュームレンダリングは微分可能（特に工夫は必要ない（自動微分でオーケー））
レンダリング方程式を放射輸送方程式に戻し、ニューラルネットワークで表される3Dシーン（ベクトル場）と微分可能なボリュームレンダリング関数で解く
ニューラル場（Neural Fields）の研究はNeRFの登場以前からも行われており、NeRFはDeepSDFのテクニックを継承している
陰関数とCGの関係 従来のCGは離散的な頂点定義によって成り立っているのに対して、陰関数（Implicit Function）によるCGは連続的な数学定義によって成り立っている
陰関数で0の値をとるサーフェスを定義し、距離場（SDF、TSDF、DeepSDF）はそのサーフェスからの距離を定義する
関数でCGが表現できる！（密に点群を用意せずとも関数でCGが表現できる点で最近注目されている）
参考：【メタサーベイ】Neural Fields</description>
    </item>
    
    <item>
      <title>CG理論 #4 シェーディング [Devlog #009]</title>
      <link>https://nishihi6.github.io/blog/posts/devlog_cg_04/</link>
      <pubDate>Sat, 30 Sep 2023 22:43:30 +0900</pubDate>
      
      <guid>https://nishihi6.github.io/blog/posts/devlog_cg_04/</guid>
      <description>CG屋さんのバイブル：Real Time Rendering Fourth Edition を読んで理解したことについてを要約します（内容の転載を避け、詳しく説明しすぎないように配慮します）
シェーディング シェーディングの基礎 シェーディングモデル 光源 平行光源 点光源 ポイント/オムニライト スポットライト その他の点光源 その他のライト シェーディングモデルの実装 評価の頻度 実装例 マテリアルシステム エイリアシングとアンチエイリアシング サンプリングとフィルタリングの理論 再構成 リサンプリング スクリーンベースのアンチエイリアシング サンプリングパターン 形態学的手法 透明度、アルファと合成 ブレンドの順番 順番に依存しない透明度 乗算済みアルファと合成 ディスプレイエンコーディング </description>
    </item>
    
    <item>
      <title>CG理論 #3 変換 [Devlog #008]</title>
      <link>https://nishihi6.github.io/blog/posts/devlog_cg_03/</link>
      <pubDate>Fri, 29 Sep 2023 23:41:38 +0900</pubDate>
      
      <guid>https://nishihi6.github.io/blog/posts/devlog_cg_03/</guid>
      <description>CG屋さんのバイブル：Real Time Rendering Fourth Edition を読んで理解したことについてを要約します（内容の転載を避け、詳しく説明しすぎないように配慮します）
変換 変換（transform） 基本的な変換 平行移動 回転 拡大縮小（スケール） せん断 変換の連結 剛体変換 法線の変換 逆行列の計算 特殊な行列変換と操作 オイラー変換 オイラー変換からのパラメータ抽出 行列の分解 任意の軸の周りの回転 クォータニオン クォータニオン クォータニオン変換 行列への変換 球面線形補間 頂点ブレンド モーフィング ジオメトリーキャッシュの再生 投影 正投影 透視投影 </description>
    </item>
    
    <item>
      <title>DirectX12の描画処理 #4 [Devlog #007]</title>
      <link>https://nishihi6.github.io/blog/posts/devlog_dx12_04/</link>
      <pubDate>Wed, 20 Sep 2023 22:48:58 +0900</pubDate>
      
      <guid>https://nishihi6.github.io/blog/posts/devlog_dx12_04/</guid>
      <description>DirectX12を使った描画処理 ポリゴンの描画 ライブラリの導入 DirectXMath DirectXMath APIは、DirextXアプリケーションに共通する一般的な線形代数およびグラフィックス演算用のSIMD対応のC++関数を提供するライブラリで、ユニバーサルWindowsプラットフォームアプリ、Xboxゲーム開発者向けに設計されている 詳細
DirectXTK DirectXTK(DirectX Tool Kit)は、マイクロソフト社が提供するDirectX 11と12に対応したゲーム開発を行うためのオープンソースのクラスライブラリであり、XNA Game Framework（2006年に発表）がC++に移植されたもの DirextXTKの詳細 ユニバーサルWindowsプラットフォーム(UWP)用ゲーム開発の詳細
NuGet NuGetは、Microsoft .NET環境で使用されるパッケージマネージャーであり、拡張子が.nupkgのzipファイルにはコンパイル済みのコード（DLL）、そのコードに関連する他のファイル、パッケージのバージョン番号などの情報が記述されているマニフェストが含まれる 詳細
DirectXMathの導入 Visual Studioのツール-&amp;gt;NuGetパッケージマネージャー-&amp;gt;ソリューションのNuGetパッケージの管理で「directxtk12_uwp」を検索して選択、プロジェクト（frameworkDX12を含む）にチェックしてインストール
stdafx.h // stdafx.h // ~ // // DirectX Took Kit #pragma comment(lib, &amp;#34;DirectXTK12.lib&amp;#34;) #include &amp;lt;SimpleMath.h&amp;gt; // ~ // Utility.h // Utility.h // ~ // namespace Math = DirectX::SimpleMath; メッシュ GraphicsフォルダにMeshフォルダを追加し、Mesh.hとMesh.cppを追加
dev-dx12-202308（プロジェクト名） └─ Source ├─ Application │ ├─ Application.h │ └─ Application.cpp ├─ Graphics │ ├─ Mesh │ │ ├─ Mesh.</description>
    </item>
    
    <item>
      <title>Unity-VR 備忘録 #1 [Devlog #006]</title>
      <link>https://nishihi6.github.io/blog/posts/devlog_unity-vr_01/</link>
      <pubDate>Wed, 06 Sep 2023 17:42:09 +0900</pubDate>
      
      <guid>https://nishihi6.github.io/blog/posts/devlog_unity-vr_01/</guid>
      <description>動作確認 開発環境 Windows10 Unity2022.3.8f1(LTS) MetaQuest2（QuestLink） 環境設定 プロジェクトの設定 レンダリングパイプライン レンダリングパイプラインにはSRP（スクリプタブルレンダーパイプライン）の中でも汎用性の高いURPを使用 URPの詳細
テンプレート 手動でVR周りの設定をするため、テンプレートはVRではなく3D（URP）を使用
パッケージ InputSystem 1.6.3 XR Interaction Toolkit 2.4.3 XR Plugin Management 4.3.3 プロジェクトセッティング Project Settings-&amp;gt;XR Plug-in ManagementのPlug-in ProvidersのOpen XRにチェックを入れる
Project Settings-&amp;gt;XR Plug-in Management-&amp;gt;OpenXRのInteraction ProfilesにOculus Touch Controller Profileを追加
StarterAssetsの追加 Package Manager-&amp;gt;XR Interaction Toolkit-&amp;gt;SamplesからStarterAssetsをインポート
シーンの設定 SampleSceneのMainCameraはVRで利用しないので消去
HierarchyにXR-&amp;gt;XR Origin(VR)を追加
XR OriginコンポーネントがアタッチされたGameObjectのXR Originに、Packages&amp;gt;XR Interaction Toolkit&amp;gt;Runtime&amp;gt;Inputs&amp;gt;InputActionManager.csをアタッチして、 Action AssetsのElement 0に、Assets&amp;gt;Samples&amp;gt;XR Interaction Toolkit&amp;gt;2.4.3&amp;gt;Starter Assets&amp;gt;XRI Default Input Actions.inputactionsを設定する ※設定されてなければ手動で設定
コントローラーの設定 ~Starter Assets&amp;gt;XRI Default Left ControllerをHierarchyのXR Origin&amp;gt;Camera Offset&amp;gt;Left ControllerのXR Controller(Action-based)コンポーネントにドラッグアンドドロップ</description>
    </item>
    
    <item>
      <title>CG理論 #2 グラフィクス処理 [Devlog #005]</title>
      <link>https://nishihi6.github.io/blog/posts/devlog_cg_02/</link>
      <pubDate>Wed, 30 Aug 2023 03:46:40 +0900</pubDate>
      
      <guid>https://nishihi6.github.io/blog/posts/devlog_cg_02/</guid>
      <description>CG屋さんのバイブル：Real Time Rendering Fourth Edition を読んで理解したことについてを要約します（内容の転載を避け、詳しく説明しすぎないように配慮します）
グラフィクス処理 グラフィクス処理ユニット（GPU） グラフィクスを高速化する歴史は、三角形と重なる各ピクセルの色を補間することから始まった
頂点処理を行う消費者用チップとして1999年に出荷されたGeForce 256が初めてのグラフィクス処理ユニット（GPU）であり、ラスタライズ専用チップと差別化する目的でその言葉が生まれた
GPUは特定のタスクに特化した複雑な固定機能パイプライン処理をもつものから、開発者が独自のアルゴリズムを実装できる柔軟性をもつものへと進化している1
GPUの利点は、狭い範囲の高度に並列化可能なタスクのセットに集中することで、頂点変換やピクセルの色計算の処理を高速化させることにある
カスタムシリコンが行う並列処理の例 z-バッファの実装 テクスチャーイメージ、バッファーへの迅速なアクセス 三角形が重なるピクセルの探索 データ並列アーキテクチャ CPU 複数のプロセッサを持つことがあるが、それぞれがほぼ直列に処理を実行し、レイテンシーを最小化するためにチップの多くがメモリーで占められている
ストール（命令の実行が遅れること）を避けるために、分岐予測や命令並べ替え、レジスタリネーミング、キャッシュプリフェッチなどを行う2
GPU よく似たデータの順序つきセット（例えば頂点やピクセルのセット）を順番に並列処理するストリームプロセッサ
それぞれのプロセッサは書き込み可能なメモリ位置の共有を行わず、可能な限り独立するように設計されているため、それぞれのプロセッサが別のプロセッサの作業の終了を待つことが無い
シングルスレッドの性能ではなく、全スレッドの合計の処理性能（スループット）を重視して最適化したプロセッサであり3、ストール発生箇所（高速なローカルメモリではなく別のリソースへアクセスするテクスチャーアクセスなど）を考慮した仕組みになっている （例えば、テクスチャフェッチ中に次のフラグメントの処理に移るなど）
実行の最適化 GPUは、命令実行ロジックをデータから分離した方式SIMD（単一命令複数データ）を利用することで、ロックステップ（多数のデータ要素を同じ命令に従って処理）する
同じシェーダープログラムによって処理される各フラグメントに対するピクセルシェーダーの呼び出し（スレッド）はワープ/ウェーブフロントと呼ばれるグループに束ねられ、GPUシェーダーコアの実行（シェーダープロセス）にスケジュールされる （例えば、32スレッドを束ねるNVIDIA GPUで2000フラグメントを処理すると、63のワープを割り当てることになる（2000/32=62.5））※本書P28 図3.1を参照
各スレッドは個々のレジスタを持ち、ワープ単位で命令を追跡する
ワープはストールする際に別のスレッドのワープとスワップ（スワップイン/アウト）を行う
他にも実行の最適化に使われる技術はいくつかある4
効率を測る シェーダープログラムの構造、特にスレッドごとに使われるレジスタの数が効率に影響し、各スレッドの処理に必要なレジスタが増えるほど、GPU上に常駐できるスレッド数は減るのでワープも減る（ストールをスワップで軽減できなくなる）
参考論文↓ シェーダーが使うレジスタ数と共有メモリの占有率（常駐するワープの数）への影響5 シェーダーが行う操作の種類による理想的な占有率の変化67
if文とループによる動的分岐は全体的な効率に影響するというスレッド発散問題がある （１つでも別の経路をとるスレッドがあると、ワープは両方の分岐を実行してそれぞれのスレッドで不要な結果を捨てなければならない）84
GPUレンダリングパイプラインの実装方法 GPUの論理モデルはAPIが提供するもので、ハードウェアベンダーによって異なる
頂点シェーダーは完全にプログラマブル テッセレーションは完全にプログラマブル
（プリミティブ（点、直線、三角形）の頂点に作用） テッセレーション・ジオメトリーシェーダーはオプション
（モバイルデバイスではプログラマブルでない場合もある） クリッピングは固定機能ハードウェアで実装される スクリーンマッピングは単純なスケールと再配置を行う
（ウィンドウとビューポートの設定による） 三角形セットアップ・三角形トラバースは固定機能ハードウェアで実装される ピクセルシェーダーは完全にプログラマブル マージはプログラマブルではないが高度に設定可能
（色、z-バッファ、ブレンド、ステンシルなどのバッファの修正） プログラマブルシェーダー 統合型シェーダーデザインは、「頂点、ピクセル、ジオメトリ、テッセレーション」に関するシェーダーが共通のプログラミングモデルを共有することを意味する（内部で同じ命令セットアーキテクチャ（ISA : Instruction Set Architecture）をもつ）
DirectXでは共通シェーダーコアが統合シェーダーアーキテクチャをもつ
シェーダープロセッサが様々な役割に使用可能であり、GPUは統合シェーダーコアによって負荷のバランスを調整することができる （例：四角形ポリゴンセットよりも三角形ポリゴンセットのほうが多くの頂点シェーダー処理が必要）
シェーダーはシェーディング言語（DirectXはHLSL、OpenGLはGLSL、NVIDIA社のCg）を使ってプログラムが可能である
HLSLは中間表現として中間言語（ILまたはDXIL）というシェーダー仮想機械のバイトコードにコンパイルしてGPUのISAに変換する
ドローコールはグラフィクスAPIを起動してグラフィクスパイプラインとシェーダーを実行させる
プログラマブルシェーダーのステージの入力には、ドローコール中は変化しない一様入力とドローコール中に変化する可変入力がある （例：テクスチャは色データの配列であり変化しないため一様入力） （例：三角形の頂点やラスタライズから発生するデータは変化するため可変入力）</description>
    </item>
    
    <item>
      <title>CMakeについてのメモ [Memo #002]</title>
      <link>https://nishihi6.github.io/blog/posts/memo_cmake_01/</link>
      <pubDate>Tue, 29 Aug 2023 15:43:33 +0900</pubDate>
      
      <guid>https://nishihi6.github.io/blog/posts/memo_cmake_01/</guid>
      <description>CV・CG系のライブラリはCMakeを使って開発されていることが多く、今後CMakeを頻繁に利用することが考えられるため、その使い方をまとめておく
CMakeの仕組み C/C++のビルドの仕組みとライブラリの使用方法 参考サイト（１）：https://qiita.com/seriru13/items/c2f5192615162c4c3f47 参考サイト（２）：https://kamino.hatenablog.com/entry/c%2B%2B-principle-of-build-library
コマンドラインでのビルドとCMakeでのビルドを比較 参考サイト（３）：https://qiita.com/shohirose/items/45fb49c6b429e8b204ac
実行ファイルの作成 静的・共有ライブラリの作成 サブディレクトリのソース 参考サイト（４）：https://qiita.com/shohirose/items/637f4b712893764a7ec1
コンパイルオプションの設定 ビルドタイプの指定 ジェネレーター式 ライブラリのリンク 参考サイト（５）：https://qiita.com/shohirose/items/d2b9c595a37b27ece607
他のライブラリの利用 実際にCMakeを利用する 参考サイト（６）：https://kamino.hatenablog.com/entry/cmake_tutorial1
CMakeのインストール Configurate &amp;amp; Generate ビルド デバッグ 参考サイト（７）：https://kamino.hatenablog.com/entry/cmake_tutorial2
階層化 参考サイト（８）：https://kamino.hatenablog.com/entry/cmake_tutorial3
CMakeプロジェクトの設定 ビルドタイプの設定 参考サイト（９）：https://kamino.hatenablog.com/entry/cmake_tutorial4
find_package 外部ライブラリの利用 外部ライブラリ利用の手順 例として、数理最適化ライブラリceres-solverとその依存ライブラリEigenとglogを利用
Eigenのインストール EigenのGitHubリポジトリをクローン
手動インストールのためC:\lib\eigen3.3.7にビルド
（git clone https://github.com/eigenteam/eigen-git-mirror.git）※今回はsourcetreeを利用 cd eigen-git-mirror git checkout 3.3.7 mkdir build cd build cmake .. -DCMAKE_INSTALL_PREFIX=&amp;#39;C:/lib/eigen3.3.7&amp;#39; cmake --build . --target install Glogのインストール GlogのGitHubリポジトリをクローン
手動インストールのためC:\lib\glog0.4.0にビルド
（git clone https://github.com/eigenteam/eigen-git-mirror.git）※今回はsourcetreeを利用 cd eigen-git-mirror git checkout 3.3.7 mkdir build cd build cmake .</description>
    </item>
    
    <item>
      <title>メモリンク集 [Memo #001]</title>
      <link>https://nishihi6.github.io/blog/posts/memo_links_01/</link>
      <pubDate>Mon, 28 Aug 2023 21:43:10 +0900</pubDate>
      
      <guid>https://nishihi6.github.io/blog/posts/memo_links_01/</guid>
      <description>私用メモ CMakeについてのメモ [Memo #002] https://nishihi6.github.io/blog/posts/memo_links_01/</description>
    </item>
    
    <item>
      <title>DirectX12の描画処理 #3 [Devlog #004]</title>
      <link>https://nishihi6.github.io/blog/posts/devlog_dx12_03/</link>
      <pubDate>Sun, 27 Aug 2023 21:44:55 +0900</pubDate>
      
      <guid>https://nishihi6.github.io/blog/posts/devlog_dx12_03/</guid>
      <description>DirectX12を使った描画処理 DirectX12の環境構築 初期化 Direct3D (D3D) Direct3D(D3D)とは、DirectXの中の3DグラフィクスAPIの一部であり、多様なグラフィクスハードウェア上でハードウェアアクセラレーションを活用したグラフィクスの表示や操作を行うことができる
D3D12Deviceは、D3D12(Direct3D 12)のインターフェースの一つで、グラフィクスハードウェアへの低レベルのアクセスを提供し、リソースの作成やコマンドのキューの管理などの主要な機能をもつ
DirectX Graphics Infrastructure (DXGI) DXGIとは、DirectXのコンポーネントの一つであり、アダプター（グラフィクスカード）、モニター、フレームバッファなどのリソース管理や、スワップチェーンの操作などのタスクを行う DXGIは、Direct3Dが多様なグラフィクスハードウェア上で一貫して動作するための基盤となる
DXGIFactoryは、DXGIの中でも主要なインターフェースの一つで、アダプター（グラフィクスカード）やモニターの情報を取得したり、スワップチェーンを作成するためのメソッドを提供する
デバイスの初期化 D3D12DeviceとDXGIFactoryは初期化に最低限必要になる
リンクするライブラリファイルd3d12.libとdxgi.libをソースコード内の記述でリンクして、d3d12.hとdxgi1_6.hをインクルードする
// stdafx.h // D3D12 #pragma comment(lib,&amp;#34;d3d12.lib&amp;#34;) #pragma comment(lib,&amp;#34;dxgi.lib&amp;#34;) #include &amp;lt;d3d12.h&amp;gt; #include &amp;lt;dxgi1_6.h&amp;gt; GraphicsフォルダにGraphicsDevice.hとGraphicsDevice.cppを追加
dev-dx12-202308（プロジェクト名） └─ Source ├─ Application │ ├─ Application.h │ └─ Application.cpp ├─ Graphics │ ├─ GraphicsDevice.h │ └─ GraphicsDevice.cpp ├─ System │ ├─ Utility │ │ ├─ Utility.h │ │ └─ Utility.cpp │ ├─ Window │ │ ├─ Window.h │ │ └─ Window.</description>
    </item>
    
    <item>
      <title>DirectX12の描画処理 #2 [Devlog #003]</title>
      <link>https://nishihi6.github.io/blog/posts/devlog_dx12_02/</link>
      <pubDate>Sat, 26 Aug 2023 18:46:37 +0900</pubDate>
      
      <guid>https://nishihi6.github.io/blog/posts/devlog_dx12_02/</guid>
      <description>DirectX12を使った描画処理 環境構築の続き プリコンパイル済みヘッダーの追加 Sourceフォルダ以下にstdafx.hとstdafx.cppと追加
dev-dx12-202308（プロジェクト名） └─ Source ├─ Application │ ├─ Application.h │ └─ Application.cpp ├─ Graphics ├─ System │ └─ Window │ ├─ Window.h │ └─ Window.cpp ├─ stdafx.h └─ stdafx.cpp プロジェクトのプロパティの設定からプリコンパイル済みヘッダーを使用(/Yu)に設定（プリコンパイル済みヘッダーファイルはstdafx.h）
プロジェクトのプロパティの設定から必ず使用されるインクルードファイルにstdafx.hを追加設定
追加したstdafx.cppのプロパティの設定からプリコンパイル済みヘッダーを作成(/Yc)に設定（プリコンパイル済みヘッダーファイルはstdafx.h）
以下コード
// stdafx.h #pragma once // 基本 #pragma comment(lib,&amp;#34;winmm.lib&amp;#34;) #define NOMINMAX #include &amp;lt;Windows.h&amp;gt; #include &amp;lt;iostream&amp;gt; #include &amp;lt;cassert&amp;gt; #include &amp;lt;wrl/client.h&amp;gt; // STL #include &amp;lt;map&amp;gt; #include &amp;lt;unordered_map&amp;gt; #include &amp;lt;unordered_set&amp;gt; #include &amp;lt;string&amp;gt; #include &amp;lt;array&amp;gt; #include &amp;lt;vector&amp;gt; #include &amp;lt;stack&amp;gt; #include &amp;lt;list&amp;gt; #include &amp;lt;iterator&amp;gt; #include &amp;lt;queue&amp;gt; #include &amp;lt;algorithm&amp;gt; #include &amp;lt;memory&amp;gt; #include &amp;lt;random&amp;gt; #include &amp;lt;fstream&amp;gt; #include &amp;lt;iostream&amp;gt; #include &amp;lt;sstream&amp;gt; #include &amp;lt;functional&amp;gt; #include &amp;lt;thread&amp;gt; #include &amp;lt;atomic&amp;gt; #include &amp;lt;mutex&amp;gt; #include &amp;lt;future&amp;gt; #include &amp;lt;fileSystem&amp;gt; #include &amp;lt;chrono&amp;gt; #define _USE_MATH_DEFINES #include &amp;lt;math.</description>
    </item>
    
    <item>
      <title>CG理論 #1 レンダリングパイプライン [Devlog #002]</title>
      <link>https://nishihi6.github.io/blog/posts/devlog_cg_01/</link>
      <pubDate>Thu, 24 Aug 2023 15:22:11 +0900</pubDate>
      
      <guid>https://nishihi6.github.io/blog/posts/devlog_cg_01/</guid>
      <description>CG屋さんのバイブル：Real Time Rendering Fourth Edition を読んで理解したことについてを要約します（内容の転載を避け、詳しく説明しすぎないように配慮します）
レンダリングパイプライン レンダリングパイプラインの主な機能は、視点（仮想カメラ）、光源、３次元オブジェクトから２次元イメージをレンダーすること
（２次元イメージ中の）オブジェクトの位置と形を決定するもの
幾何学形状（ジオメトリ） 環境特性 カメラ配置 （２次元イメージ中の）オブジェクトの見た目を決定するもの
マテリアル特性 光源 表面テクスチャ シェーディングの式 アーキテクチャ レンダリングパイプラインはいくつかのステージからなり1、スピードアップを主な目的として並列に実行される
主なステージ（各々でさらにいくつかのサブステージで構成される）
アプリケーション（衝突検出、アニメーション、物理シミュレーションなど） ジオメトリ処理（座標変換、投影などの幾何学的処理） ラスタライズ（3つの頂点から三角形の内側のピクセルを求める） ピクセル処理（ピクセル単位での処理（色や深度など）） フレーム間に実行する計算の複雑さによってフレーム/秒が変化し、これによりレンダリングの性能を表すことが一般的
アプリケーション 開発者はこのアプリケーションステージで何が起こるのかを制御する
アプリによって一番の違いが見られるのはレンダリングパイプラインの中でもこのアプリケーションステージであると思われる
アプリケーションの作業は基本的にCPU上で実行するが、コンピュートシェーダーを使ってGPU上で実行することもある
レンダーすべきジオメトリ（点、直線、三角形）をジオメトリ処理ステージに渡すのが主なタスク
ユーザからの入力情報を扱ったり、描画する必要がないポリゴンを求めるカリングアルゴリズムを実行したりと様々な処理を行う
[DX12] Input-Assembler（頂点情報やインデックス情報の入力） Input-Assembler（入力アセンブラー）は頂点情報だけではなく、&amp;ldquo;どの3頂点を組み合わせて三角形を作るのか&amp;quot;という情報などが入力されるステージ
ポリゴンの表示のためには、数値（バイトの塊）を解釈するための頂点レイアウトとインデックス情報、そしてもちろん頂点情報（バイトの塊）が入力情報として必要になる
ジオメトリ処理 ジオメトリ処理ステージでは、幾何学形状（ジオメトリ）を三角形単位と頂点単位で操作する
ジオメトリ処理ステージのサブステージ（機能ステージ）
頂点シェーディング 投影 クリッピング スクリーンマッピング 頂点シェーディング 頂点シェーディングでは、頂点位置を計算し、頂点出力データにもたせる法線やテクスチャ座標などを評価する
頂点シェーダーのもともとの仕組み2
各頂点の位置と法線に光源を適用して色を計算 頂点の色を三角形上で補間 頂点シェーダーは各頂点に関連付けたデータの設定を行う
頂点の計算
モデル空間にモデルが存在（モデルのいずれかの頂点や近傍などに原点をとる） モデル変換により、モデルごとにモデル空間内での位置と向きが決まる オブジェクトはモデル座標（＝ローカル座標：モデル空間の座標）をもち、モデル変換が適用されると、ワールド空間内でのワールド座標が決まる カメラ空間での頂点の計算
ビュー変換により、カメラを原点としたカメラ空間での座標が決まる
（視線がZ軸（負or正）方向、上がY軸、右がX軸 ※API依存） モデル変換とビュー変換のいずれも4×4行列として実装できる
シェーディング
シェーディングでは、各オブジェクトのマテリアルとそれを照らす光源による効果を決定する見た目のモデル化を行う
各点におけるシェーディングの式の計算は、ジオメトリ処理の間に行うものもあれば、ピクセル単位の処理で行うものもある
続くステージに出力する色、ベクトル、テクスチャ座標、その他のシェーディングデータは、頂点ごとに格納されたマテリアルデータ（点の位置、色、シェーディング式の評価に必要な数値情報など）を用いて算出される
投影からクリッピングへ
正準ビューボリュームは、端点が(-1,-1,-1)と(1,1,1)にある単位立方体で、0 &amp;lt;= z &amp;lt;= 1等のボリュームを使って定義される
頂点シェーダーが行う投影
正投影（平行投影） -&amp;gt; 直方体のから単位立方体に変換 透視投影 -&amp;gt; ピラミッド形状の錐台から単位立方体に変換 その他：斜投影や不等角投影など 変換はいずれも4×4行列として実装できる</description>
    </item>
    
    <item>
      <title>DirectX12の描画処理 #1 [Devlog #001]</title>
      <link>https://nishihi6.github.io/blog/posts/devlog_dx12_01/</link>
      <pubDate>Wed, 23 Aug 2023 01:19:34 +0900</pubDate>
      
      <guid>https://nishihi6.github.io/blog/posts/devlog_dx12_01/</guid>
      <description>DirectX12を使った描画処理 環境構築 VisualStudio2022でプロジェクトの作成 (新規作成-&amp;gt;プロジェクト-&amp;gt;)新しいプロジェクトの作成-&amp;gt;空のプロジェクト プロジェクト名（ソリューション名）と保存パスを指定 &amp;ldquo;ソリューションとプロジェクトを同じディレクトリに配置する&amp;quot;にチェック 作成 ソリューションのプロパティを変更 構成プロパティ-&amp;gt;リンカー-&amp;gt;システム-&amp;gt;サブシステム &amp;ldquo;コンソール&amp;quot;から&amp;quot;Windows&amp;quot;に変更 以下のようにフォルダを構成 新しいフォルダを作成 フォルダに新しい項目を追加（以降説明省略） Applicationフィルターに新しい項目を追加 .cpp 名前：Application.cpp 場所：Source/Application（フォルダを作成） .h 名前：Application.h 場所：Source/Application dev-dx12-202308（プロジェクト名） └─ Source ├─ Application │ ├─ Application.h │ └─ Application.cpp └─ Graphics Application.h Applicationクラス ウィンドウ作成の実行を行うExecute()関数を宣言
Applicationクラスオブジェクトを生成するInstance()関数を定義する（シングルトン：クラスのインスタンスが１つしか存在しないことを保証する）
// Application.h // Applicationクラス class Application { public: void Execute(); static Application&amp;amp; Instance() { static Application instance; return instance; } private: Window m_window; Application() {} } Application.cpp Windows.hのインクルード WindowsプログラムのためにWindows.hをインクルード（Windowsプログラムの型や構造体、定数、ファンクションコールが定義されている） （※Windows.hとWindow.hの混同に注意）
Windowsプログラムにmain()関数は無く、WinMain()関数からプログラムが開始される
WinMain()関数 WINAPIはWindows.hでWin32 API 関数を呼び出すときの規約として定義される。#define WINAPI __stdcall</description>
    </item>
    
    <item>
      <title>My First Post（記事作成に関する備忘録）</title>
      <link>https://nishihi6.github.io/blog/posts/my-first-post/</link>
      <pubDate>Tue, 22 Aug 2023 06:05:10 +0900</pubDate>
      
      <guid>https://nishihi6.github.io/blog/posts/my-first-post/</guid>
      <description>&lt;p&gt;Hello world! It&amp;rsquo;s me!&lt;/p&gt;
 &lt;img src=&#34;../img/itsme.jpg&#34; width=&#34;200px&#34;&gt;
&lt;p&gt;以下、記事作成のマニュアル&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
